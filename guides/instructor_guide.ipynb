{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI & Machine Learning (KAN-CINTO4003U) - Copenhagen Business School | Spring 2025**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"media/instructor_header.png\" alt=\"LLM\" width=\"800\"/> <br>\n",
    "Image from <a href=\"https://medium.com/thoughts-on-machine-learning\">Thoughts on Machine Learning</a>'s \"<i><a href=\"https://medium.com/thoughts-on-machine-learning/drop-langchain-instructor-is-all-you-need-for-your-llm-based-applications-aed13e9b908b\">Drop LangChain, Instructor Is All You Need For Your LLM-Based Applications\"</a><br>by FS Ndzomga</i>. Copyright Â© 2025. All rights reserved.\n",
    "</p>\n",
    "\n",
    "***\n",
    "Sources: <br>\n",
    "- [Drop LangChain, Instructor Is All You Need For Your LLM-Based Applications (Medium)](https://medium.com/thoughts-on-machine-learning/drop-langchain-instructor-is-all-you-need-for-your-llm-based-applications-aed13e9b908b)\n",
    "\n",
    "\n",
    "# Instructor\n",
    "\n",
    "[Instructor](https://python.useinstructor.com/#getting-started) is a python package that makes it easy to get structured data like JSON from LLMs like GPT-3.5, GPT-4, GPT-4-Vision, and open-source models including Mistral/Mixtral, Ollama, and llama-cpp-python - and WatsonX.ai models as we are using in this course. It stands out for its simplicity, transparency, and user-centric design, built on top of Pydantic. Instructor helps you manage validation context, retries with Tenacity, and streaming Lists and Partial responses.\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| Simple API with Full Prompt Control | Instructor provides a straightforward API that gives you complete ownership and control over your prompts. This allows for fine-tuned customization and optimization of your LLM interactions. |\n",
    "| Multi-Language Support | Simplify structured data extraction from LLMs with type hints and validation. Supports Python, TypeScript, Ruby, Go, Elixir, and Rust. |\n",
    "| Reasking and Validation | Automatically reask the model when validation fails, ensuring high-quality outputs. Leverage Pydantic's validation for robust error handling. |\n",
    "| Streaming Support | Stream partial results and iterables with ease, allowing for real-time processing and improved responsiveness in your applications. |\n",
    "| Powered by Type Hints | Leverage Pydantic for schema validation, prompting control, less code, and IDE integration. |\n",
    "| Simplified LLM Interactions | Support for OpenAI, Anthropic, Google, Vertex AI, Mistral/Mixtral, Ollama, llama-cpp-python, Cohere, LiteLLM. |\n",
    "\n",
    "Simply put, we can use Instructor to extract structured data from LLMs, instead of just plain test. In practically all cases, we want more than just a text dump from an LLM, and postprocessing LLM outputs can be a tedious, error-prone task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LiteLLM\n",
    "\n",
    "One of the disadvantages of working with different LLM vendors (Azure AI, OpenAI, Anthropic, WatsonX etc.) is that they all have different API schemas. This means that we often have to build platform-specificer adapters if we are working with models from multiple places. [LiteLLM](https://docs.litellm.ai/) is an open source package that enable us to call [100+ LLMs from 56 providers](https://docs.litellm.ai/docs/providers) using the standard OpenAI Input/Output format. Behind the scenes, LiteLLM translates inputs to any provider's completion, embedding, and image_generation endpoints, and they ensures that we get a response that follows the OpenAI API schema back. \n",
    "\n",
    "You can read exactly how the [API works for WatsonX.ai here](https://docs.litellm.ai/docs/providers/watsonx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it together\n",
    "\n",
    "With LiteLLM we can initialize the WatsonX.ai models and then use Instructor to extract structured data from the LLMs. This is a powerful combination that allows us to work with multiple LLM vendors without having to worry about the differences in their APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# built-in libraries\n",
    "from typing import TypeVar, Literal, Any\n",
    "\n",
    "# litellm libraries\n",
    "import litellm\n",
    "from litellm import completion\n",
    "from instructor import Mode, from_litellm\n",
    "\n",
    "# misc libraries\n",
    "from decouple import config\n",
    "from pydantic import BaseModel, Field, create_model\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading our WatsonX.ai credentials again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_file_path = \"/Users/henrikjacobsen/Desktop/CBS/Semester 2/Artifical Intelligence and Machine Learning/apikey.json\"\n",
    "\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "WX_API_KEY = data.get(\"apikey\")\n",
    "\n",
    "if WX_API_KEY:\n",
    "    print(\"API Key loaded successfully!\")\n",
    "else:\n",
    "    print(\"Error: API Key not found in JSON file.\")\n",
    "\n",
    "WX_PROJECT_ID = \"0a2386df-d12c-40ee-bda2-190a5c6cc1fd\"\n",
    "WX_API_URL = \"https://us-south.ml.cloud.ibm.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call a model from WatsonX.ai with LiteLLM first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Call WATSONX `/text/chat` endpoint - supports function calling\n",
    "response = completion(\n",
    "  model=\"watsonx/meta-llama/llama-3-1-8b-instruct\",\n",
    "  messages=[{ \"content\": \"what is your favorite colour?\",\"role\": \"user\"}],\n",
    "  project_id=WX_PROJECT_ID,\n",
    "  api_key=WX_API_KEY,\n",
    "  base_url=WX_API_URL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-f858359f3fd8ac20ee188272ec67eb6e', created=1744213013, model='watsonx/meta-llama/llama-3-1-8b-instruct', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"I don't have a personal preference or emotions, so I don't have a favorite color. However, I can help you explore colors and their meanings or properties if you're interested.\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=38, prompt_tokens=41, total_tokens=79, completion_tokens_details=None, prompt_tokens_details=None), model_id='meta-llama/llama-3-1-8b-instruct', model_version='3.1.0', created_at='2025-04-09T15:36:53.803Z', system={'warnings': [{'message': 'This model is a Non-IBM Product governed by a third-party license that may impose use restrictions and other obligations. By using this model you agree to its terms as identified in the following URL.', 'id': 'disclaimer_warning', 'more_info': 'https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-models.html?context=wx'}, {'message': \"Model 'meta-llama/llama-3-1-8b-instruct' is in deprecated state from 2025-01-22. It will be in withdrawn state from 2025-05-30. IDs of alternative models: llama-3-2-11b-vision-instruct.\", 'id': 'deprecation_warning', 'more_info': 'https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-model-lifecycle.html?context=wx&audience=wdp'}, {'message': \"The value of 'max_tokens' for this model was set to value 1024\", 'id': 'unspecified_max_token', 'additional_properties': {'limit': 0, 'new_value': 1024, 'parameter': 'max_tokens', 'value': 0}}]})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't have a personal preference or emotions, so I don't have a favorite color. However, I can help you explore colors and their meanings or properties if you're interested.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Call WATSONX `/text/generation` endpoint - not all models support /chat route. \n",
    "response = completion(\n",
    "  model=\"watsonx/ibm/granite-3-2-8b-instruct\",\n",
    "  messages=[{ \"content\": \"Write a haiku about the singularity\",\"role\": \"user\"}],\n",
    "  project_id=WX_PROJECT_ID,\n",
    "  api_key=WX_API_KEY,\n",
    "  base_url=WX_API_URL,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silicon heart beats,\n",
      "Binary dawn whispers waves,\n",
      "Singularity blooms.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now, let's see how we can use `instructor` to pair with this neat interface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "litellm.drop_params = True  # watsonx.ai doesn't support `json_mode`\n",
    "client = from_litellm(completion, mode=Mode.JSON)  # create an instructor client from litellm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to create a so-called `response_model`. This is a Pydantic model that defines the structure of the data we want to extract from the LLM. This is done using `pydantic` - another really great library for data validation and settings management. Pydantic is used by `instructor` to validate the data we get back from the LLM, and it also helps us to define the structure of the data we want to extract.\n",
    "\n",
    "Consider the example `Response` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a response model\n",
    "class Response(BaseModel): # <--- BaseModel is a Pydantic class\n",
    "\n",
    "    # ask the LLM to return a short reasoning - Remember how reasoning can help LLMs?\n",
    "    reasoning : str = Field(description=\"The short reasoning behind the answer\")\n",
    "    # ask the LLM to return the answer as a separate field\n",
    "    answer : float = Field(description=\"Your answer to the question as a float\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we are asking the LLM for two separate outputs:\n",
    "\n",
    "1. Reasoning of type `str` with a description added to give the LLM more context.\n",
    "2. The answer of type `str`, also with a description added to give the LLM more context.\n",
    "\n",
    "If we wanted to, for example, extract a reasoning an a float score, we could have done something like this:\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Response(BaseModel):\n",
    "    reasoning: str = Field(description=\"The reasoning behind the answer\")\n",
    "    score: float = Field(description=\"The score of the answer\")\n",
    "```\n",
    "\n",
    "We could even create nested models, like so:\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Reasoning(BaseModel):\n",
    "    reasoning: str = Field(description=\"The reasoning behind the answer\")\n",
    "    score: float = Field(description=\"The score of the answer\")\n",
    "\n",
    "class Response(BaseModel):\n",
    "    reasoning: Reasoning = Field(description=\"The reasoning behind the answer\")\n",
    "    answer: str = Field(description=\"The answer to the question\")\n",
    "```\n",
    "\n",
    "Now, it should be noted that if we create more complex models, we might run into issues with smaller models - and even some bigger ones. Effectively, putting the answer into a response model can be considered an additional task we are asking the LLM to perform. Hence, we generally want to keep the response models as simple as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we then use the response model we have created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "ename": "InstructorRetryException",
     "evalue": "litellm.APIConnectionError: API key is required\nTraceback (most recent call last):\n  File \"/Applications/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/litellm/main.py\", line 2714, in completion\n    response = watsonx_chat_completion.completion(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Applications/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/litellm/llms/watsonx/chat/handler.py\", line 46, in completion\n    headers = watsonx_chat_transformation.validate_environment(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Applications/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/litellm/llms/watsonx/common_utils.py\", line 187, in validate_environment\n    token = _generate_watsonx_token(api_key=api_key, token=token)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Applications/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/litellm/llms/watsonx/common_utils.py\", line 75, in _generate_watsonx_token\n    token = generate_iam_token(api_key)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Applications/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/litellm/llms/watsonx/common_utils.py\", line 43, in generate_iam_token\n    raise ValueError(\"API key is required\")\nValueError: API key is required\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/litellm/main.py:2714\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m   2713\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m custom_llm_provider == \u001b[33m\"\u001b[39m\u001b[33mwatsonx\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2714\u001b[39m     response = \u001b[43mwatsonx_chat_completion\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2715\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2716\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2717\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2718\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2719\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2720\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2721\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2722\u001b[39m \u001b[43m        \u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m=\u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2723\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2724\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2725\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2726\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2727\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m   2728\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2729\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pass AsyncOpenAI, OpenAI client\u001b[39;49;00m\n\u001b[32m   2730\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2731\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwatsonx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2732\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2733\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m custom_llm_provider == \u001b[33m\"\u001b[39m\u001b[33mwatsonx_text\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/litellm/llms/watsonx/chat/handler.py:46\u001b[39m, in \u001b[36mWatsonXChatHandler.completion\u001b[39m\u001b[34m(self, model, messages, api_base, custom_llm_provider, custom_prompt_dict, model_response, print_verbose, encoding, api_key, logging_obj, optional_params, acompletion, litellm_params, headers, logger_fn, timeout, client, custom_endpoint, streaming_decoder, fake_stream)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m## UPDATE HEADERS\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m headers = \u001b[43mwatsonx_chat_transformation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m## UPDATE PAYLOAD (optional params)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/litellm/llms/watsonx/common_utils.py:187\u001b[39m, in \u001b[36mIBMWatsonXMixin.validate_environment\u001b[39m\u001b[34m(self, headers, model, messages, optional_params, api_key, api_base)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m     token = \u001b[43m_generate_watsonx_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     \u001b[38;5;66;03m# build auth headers\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/litellm/llms/watsonx/common_utils.py:75\u001b[39m, in \u001b[36m_generate_watsonx_token\u001b[39m\u001b[34m(api_key, token)\u001b[39m\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m token\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m token = \u001b[43mgenerate_iam_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m token\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/litellm/llms/watsonx/common_utils.py:43\u001b[39m, in \u001b[36mgenerate_iam_token\u001b[39m\u001b[34m(api_key, **params)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAPI key is required\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     44\u001b[39m headers[\u001b[33m\"\u001b[39m\u001b[33mAccept\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mapplication/json\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: API key is required",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAPIConnectionError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/instructor/retry.py:168\u001b[39m, in \u001b[36mretry_sync\u001b[39m\u001b[34m(func, response_model, args, kwargs, context, max_retries, strict, mode, hooks)\u001b[39m\n\u001b[32m    167\u001b[39m hooks.emit_completion_arguments(*args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m response = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m hooks.emit_completion_response(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/litellm/utils.py:1235\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1232\u001b[39m     logging_obj.failure_handler(\n\u001b[32m   1233\u001b[39m         e, traceback_exception, start_time, end_time\n\u001b[32m   1234\u001b[39m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1235\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/litellm/utils.py:1113\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1112\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1113\u001b[39m result = \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1114\u001b[39m end_time = datetime.datetime.now()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/litellm/main.py:3144\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, thinking, **kwargs)\u001b[39m\n\u001b[32m   3142\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   3143\u001b[39m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3144\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3145\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3146\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3147\u001b[39m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3148\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3150\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2214\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2213\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mlitellm_response_headers\u001b[39m\u001b[33m\"\u001b[39m, litellm_response_headers)\n\u001b[32m-> \u001b[39m\u001b[32m2214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   2215\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2190\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2189\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2190\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(\n\u001b[32m   2191\u001b[39m                 message=\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2192\u001b[39m                     \u001b[38;5;28mstr\u001b[39m(original_exception), traceback.format_exc()\n\u001b[32m   2193\u001b[39m                 ),\n\u001b[32m   2194\u001b[39m                 llm_provider=custom_llm_provider,\n\u001b[32m   2195\u001b[39m                 model=model,\n\u001b[32m   2196\u001b[39m                 request=httpx.Request(\n\u001b[32m   2197\u001b[39m                     method=\u001b[33m\"\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m\"\u001b[39m, url=\u001b[33m\"\u001b[39m\u001b[33mhttps://api.openai.com/v1/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2198\u001b[39m                 ),  \u001b[38;5;66;03m# stub the request\u001b[39;00m\n\u001b[32m   2199\u001b[39m             )\n\u001b[32m   2200\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2201\u001b[39m     \u001b[38;5;66;03m# LOGGING\u001b[39;00m\n",
      "\u001b[31mAPIConnectionError\u001b[39m: litellm.APIConnectionError: API key is required\nTraceback (most recent call last):\n  File \"/Applications/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/litellm/main.py\", line 2714, in completion\n    response = watsonx_chat_completion.completion(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Applications/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/litellm/llms/watsonx/chat/handler.py\", line 46, in completion\n    headers = watsonx_chat_transformation.validate_environment(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Applications/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/litellm/llms/watsonx/common_utils.py\", line 187, in validate_environment\n    token = _generate_watsonx_token(api_key=api_key, token=token)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Applications/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/litellm/llms/watsonx/common_utils.py\", line 75, in _generate_watsonx_token\n    token = generate_iam_token(api_key)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Applications/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/litellm/llms/watsonx/common_utils.py\", line 43, in generate_iam_token\n    raise ValueError(\"API key is required\")\nValueError: API key is required\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRetryError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/instructor/retry.py:163\u001b[39m, in \u001b[36mretry_sync\u001b[39m\u001b[34m(func, response_model, args, kwargs, context, max_retries, strict, mode, hooks)\u001b[39m\n\u001b[32m    162\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mattempt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mattempt\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/tenacity/__init__.py:443\u001b[39m, in \u001b[36mBaseRetrying.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/tenacity/__init__.py:376\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/tenacity/__init__.py:419\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m retry_exc.reraise()\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[31mRetryError\u001b[39m: RetryError[<Future at 0x179befd50 state=finished raised APIConnectionError>]",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mInstructorRetryException\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      2\u001b[39m prompt = \u001b[33m\"\"\"\u001b[39m\u001b[33mYou are a cat expert. Answer the following question about cats:\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33mQ: What is the average lifespan of a cat?\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[33mProvide your answer as an object of Response\u001b[39m\u001b[33m\"\"\"\u001b[39m \u001b[38;5;66;03m# <-- We ask the model to return the answer as an object of Response\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# make a request to the LLM\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# <- Use the client we just created\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwatsonx/ibm/granite-3-2-8b-instruct\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# <--- model name from watsonx.ai\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m                \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m                    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m                    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# <- Our prompt\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m                \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproject_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mWX_PROJECT_ID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# <- Our credentials\u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m            \u001b[49m\u001b[43mapikey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mWX_API_KEY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m            \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mWX_API_URL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresponse_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mResponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# <- Inform the LLM of the response model\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/instructor/client.py:180\u001b[39m, in \u001b[36mInstructor.create\u001b[39m\u001b[34m(self, response_model, messages, max_retries, validation_context, context, strict, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    169\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    170\u001b[39m     response_model: \u001b[38;5;28mtype\u001b[39m[T] | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    176\u001b[39m     **kwargs: Any,\n\u001b[32m    177\u001b[39m ) -> T | Any | Awaitable[T] | Awaitable[Any]:\n\u001b[32m    178\u001b[39m     kwargs = \u001b[38;5;28mself\u001b[39m.handle_kwargs(kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidation_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/instructor/patch.py:193\u001b[39m, in \u001b[36mpatch.<locals>.new_create_sync\u001b[39m\u001b[34m(response_model, validation_context, context, max_retries, strict, hooks, *args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m response_model, new_kwargs = handle_response_model(\n\u001b[32m    188\u001b[39m     response_model=response_model, mode=mode, **kwargs\n\u001b[32m    189\u001b[39m )  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    191\u001b[39m new_kwargs = handle_templating(new_kwargs, mode=mode, context=context)\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m response = \u001b[43mretry_sync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/instructor/retry.py:194\u001b[39m, in \u001b[36mretry_sync\u001b[39m\u001b[34m(func, response_model, args, kwargs, context, max_retries, strict, mode, hooks)\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m RetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    193\u001b[39m     logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRetry error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InstructorRetryException(\n\u001b[32m    195\u001b[39m         e.last_attempt._exception,\n\u001b[32m    196\u001b[39m         last_completion=response,\n\u001b[32m    197\u001b[39m         n_attempts=attempt.retry_state.attempt_number,\n\u001b[32m    198\u001b[39m         \u001b[38;5;66;03m#! deprecate messages soon\u001b[39;00m\n\u001b[32m    199\u001b[39m         messages=extract_messages(\n\u001b[32m    200\u001b[39m             kwargs\n\u001b[32m    201\u001b[39m         ),  \u001b[38;5;66;03m# Use the optimized function instead of nested lookups\u001b[39;00m\n\u001b[32m    202\u001b[39m         create_kwargs=kwargs,\n\u001b[32m    203\u001b[39m         total_usage=total_usage,\n\u001b[32m    204\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mInstructorRetryException\u001b[39m: litellm.APIConnectionError: API key is required\nTraceback (most recent call last):\n  File \"/Applications/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/litellm/main.py\", line 2714, in completion\n    response = watsonx_chat_completion.completion(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Applications/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/litellm/llms/watsonx/chat/handler.py\", line 46, in completion\n    headers = watsonx_chat_transformation.validate_environment(\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Applications/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/litellm/llms/watsonx/common_utils.py\", line 187, in validate_environment\n    token = _generate_watsonx_token(api_key=api_key, token=token)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Applications/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/litellm/llms/watsonx/common_utils.py\", line 75, in _generate_watsonx_token\n    token = generate_iam_token(api_key)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Applications/anaconda3/envs/aiml25-ma3/lib/python3.11/site-packages/litellm/llms/watsonx/common_utils.py\", line 43, in generate_iam_token\n    raise ValueError(\"API key is required\")\nValueError: API key is required\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define a prompt\n",
    "prompt = \"\"\"You are a cat expert. Answer the following question about cats:\n",
    "Q: What is the average lifespan of a cat?\n",
    "Provide your answer as an object of Response\"\"\" # <-- We ask the model to return the answer as an object of Response\n",
    "\n",
    "# make a request to the LLM\n",
    "response = client.chat.completions.create( # <- Use the client we just created\n",
    "            model=\"watsonx/ibm/granite-3-2-8b-instruct\", # <--- model name from watsonx.ai\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,  # <- Our prompt\n",
    "                }\n",
    "            ],\n",
    "            project_id=WX_PROJECT_ID, # <- Our credentials\n",
    "            apikey=WX_API_KEY,\n",
    "            api_base=WX_API_URL,\n",
    "            response_model=Response, # <- Inform the LLM of the response model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(reasoning='The average lifespan of a cat is estimated by various sources, including the American Veterinary Medical Association and the American Society for the Prevention of Cruelty to Animals. These sources suggest that the average indoor cat lives between 13 and 17 years, while an outdoor cat typically lives between 3 to 5 years due to various dangers.', answer=15.5)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The average lifespan of a cat is estimated by various sources, including the American Veterinary Medical Association and the American Society for the Prevention of Cruelty to Animals. These sources suggest that the average indoor cat lives between 13 and 17 years, while an outdoor cat typically lives between 3 to 5 years due to various dangers.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.reasoning # <- Access the reasoning field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.5"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.answer # <- Access the answer field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going further, if we want the model to **only** be able to choose one of *n* answers, we can use the type `Literal`. This is a type hint that allows us to specify that the value of a field must be one of a set of literal values. For example, if we want the model to only be able to choose between \"Yes\" and \"No\", we can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(answer='No')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a response model\n",
    "class Response(BaseModel): # <--- BaseModel is a Pydantic class\n",
    "\n",
    "    # ask the LLM to return a short reasoning - Remember how reasoning can help LLMs?\n",
    "    answer : Literal[\"Yes\", \"No\"] = Field(description=\"Your answer to the question\")\n",
    "\n",
    "prompt = \"\"\"You are a cat expert. Answer the following question about cats:\n",
    "\n",
    "Q: Is it true that cats have nine lives?\n",
    "\n",
    "Provide your answer as an object of Response\"\"\" # <-- We ask the model to return the answer as an object of Response\n",
    "\n",
    "# make a request to the LLM\n",
    "response = client.chat.completions.create( # <- Use the client we just created\n",
    "            model=\"watsonx/ibm/granite-3-2-8b-instruct\", # <--- model name from watsonx.ai\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,  # <- Our prompt\n",
    "                }\n",
    "            ],\n",
    "            project_id=WX_PROJECT_ID, # <- Our credentials\n",
    "            apikey=WX_API_KEY,\n",
    "            api_base=WX_API_URL,\n",
    "            response_model=Response, # <- Inform the LLM of the response model\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty neat, right?\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we don't want to define response models for every call we make to an LLM? `pydantic` (and therefore `instructor`) supports *dynamic* response models, via the `create_model` function. \n",
    "\n",
    "We can use that like shown below. Note that we have to define the type (i.e. str, int, float or bool) of each response field and add a `Field` object as well. The Field object can be used to define default values, add descriptions for the LLM etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_model = create_model(\n",
    "    \"MyResponseModel\", \n",
    "    reasoning=(str, Field(description=\"The short reasoning behind the answer\")),\n",
    "    answer=(str, Field(description=\"Your answer to the question\")),\n",
    "    __base__=BaseModel\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyResponseModel(reasoning='what the LLM would reason about', answer='what the LLM would answer')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_model(reasoning=\"what the LLM would reason about\", answer=\"what the LLM would answer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our life even easier, here is a class - `LLMCaller` that will do everything we just did for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseResponse(BaseModel):\n",
    "    \"\"\"A default response model that defines a single \n",
    "    field `answer` to store the response from the LLM.\n",
    "    We will use this when there is no need to create\n",
    "    a custom response model.\"\"\"\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define a type variable for the response model\n",
    "# this you can ignore for now - it is just for type hinting\n",
    "ResponseType = TypeVar('ResponseType', bound=BaseModel)\n",
    "\n",
    "\n",
    "class LLMCaller:\n",
    "    \"\"\" A class to interact with an LLM  using the LiteLLM and Instructor\n",
    "    libraries. This class is designed to simplify the process of sending\n",
    "    prompts to an LLM and receiving structured responses. \"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str, project_id: str, api_url: str, model_id: str, params: dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Initializes the LLMCaller instance with the necessary credentials and configuration.\n",
    "\n",
    "        Args:\n",
    "            api_key (str): The API key for authenticating with the LLM service.\n",
    "            project_id (str): The project ID associated with the LLM service.\n",
    "            api_url (str): The base URL for the LLM service API.\n",
    "            model_id (str): The identifier of the specific LLM model to use.\n",
    "            params (dict[str, Any]): Additional parameters to configure the LLM's behavior.\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.project_id = project_id\n",
    "        self.api_url = api_url\n",
    "        self.model_id = model_id\n",
    "        self.params = params\n",
    "\n",
    "        # Boilerplate: Configure LiteLLM to drop unsupported parameters for Watsonx.ai\n",
    "        litellm.drop_params = True\n",
    "        # Boilerplate: Create an Instructor client for pydantic-based interactions with the LLM\n",
    "        self.client = from_litellm(completion, mode=Mode.JSON)\n",
    "\n",
    "    def create_response_model(self, title: str, fields: dict) -> ResponseType:\n",
    "        \"\"\" Dynamically creates a Pydantic response model for the LLM's output.\n",
    "        Args:\n",
    "            title (str): The name of the response model.\n",
    "            fields (dict): A dictionary defining the fields of the response model.\n",
    "                           Keys are field names, and values are tuples of (type, Field).\n",
    "\n",
    "        Returns:\n",
    "            ResponseType: A dynamically created Pydantic model class.\n",
    "        \"\"\"\n",
    "        return create_model(title, **fields, __base__=BaseResponse)\n",
    "\n",
    "    def invoke(self, prompt: str, response_model: ResponseType = BaseResponse, **kwargs) -> ResponseType:\n",
    "        \"\"\" Sends a prompt to the LLM and retrieves a structured response.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): The input prompt to send to the LLM.\n",
    "            response_model (ResponseType): The Pydantic model to structure the LLM's response.\n",
    "                                           Defaults to BaseResponse.\n",
    "            **kwargs: Additional arguments to pass to the LLM client.\n",
    "\n",
    "        Returns:\n",
    "            ResponseType: The structured response from the LLM, parsed into the specified response model.\n",
    "        \"\"\"\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_id,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt + \"\\n\\n\" + f\"Provide your answer as an object of {type(response_model)}\",\n",
    "                    # notice how we hardcode instructions on the responde model type for the llm\n",
    "                    # so we don't have to repeat it in the prompt\n",
    "                }\n",
    "            ],\n",
    "            project_id=self.project_id,\n",
    "            apikey=self.api_key,\n",
    "            api_base=self.api_url,\n",
    "            response_model=response_model,\n",
    "            **kwargs\n",
    "        )\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LLMCaller(\n",
    "    api_key=WX_API_KEY,  # <- Our credentials\n",
    "    project_id=WX_PROJECT_ID,\n",
    "    api_url=WX_API_URL,\n",
    "    model_id=\"watsonx/meta-llama/llama-3-3-70b-instruct\",  # <- model name from watsonx.ai\n",
    "    params={GenParams.MAX_NEW_TOKENS: 100}  # <- additional parameters for the LLM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseResponse(answer='A good name for a bee could be Buzz or Honey.')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"What is a good name for a bee?\")  # call with no response model - meaning we will use the default one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we want to feed in our dynamic response model, we can do that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A good name for a bee could be Buzzina\n",
      "The name Buzzina is a play on the word 'buzz', which is the sound bees make when they fly. It's a cute and memorable name that suits a busy and energetic bee\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(\n",
    "    prompt=\"What is a good name for a bee? Think carefully.\", \n",
    "    response_model=model.create_response_model(  # create a response model dynamically\n",
    "        \"BeeName\", \n",
    "        {\n",
    "            \"reasoning\": (str, Field(...)),\n",
    "            \"bee_name\": (str, Field\n",
    "                (\n",
    "                    ...,\n",
    "                    description=\"The name of the bee.\"\n",
    "                )\n",
    "            )\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "print(response.answer)\n",
    "print(response.reasoning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you see how valuable the combination of `instructor` and `litellm` can be."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml25-ma3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
